{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we will build input Embeddings\n",
    "# Here word are mapped to input IDs and input IDs are mappeed to 512 integer vector called Embeeding Vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word into IDs (Position of each word in the vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    \n",
    "    #Given a number it will return same vector every time.  This is what Embedding does It is mapping between number and vector of size 512. \n",
    "    # Here 512 vector size is d_model\n",
    "    def __init__(self, d_model:int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        #embdding is dictionary it maps number to the same vector every time.  This vector is learned by the model.\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    # In the papwer we multiply embdding layer weights by sqrt(d_model)\n",
    "    def forward(self,x):\n",
    "        return self.embeddings(x) * math.sqrt(self.d_model)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positional Encoding:  List of senstennce mapped to 512 integer vector embedding.  \n",
    "# We want to convey the model position of each of the word in the sentence.\n",
    "# To find positional encoding we will add another vector to the embedding vector using sine and cosine formula.  \n",
    "# This will give context of position of each word in the sentence\n",
    "# Create a vector same size as embedding vector with context add them to the embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    #d_model is the size of vector the position encoding should be \n",
    "    # seq_len is MAX lenght of the sentence.  Since we have to create positonal endoding for each word in the sentence.\n",
    "    #dropout is added to prevent overfitting.\n",
    "    def __init__(self, d_model, seq_len:int, dropout:float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout =nn.Dropout(dropout)\n",
    "        \n",
    "        # For positional encoding we will first build matrix of size seq_len * d_model\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Take vector of length seq_len in the even position we apply sine function formula and odd position we apply cosine function formula.\n",
    "        #Vector position that map the positon of each word inside the sentence. Create a vector of length seq_len.\n",
    "        #unsqueze function Returns a new tensor with a dimension of size one inserted at the specified position. \n",
    "        # torch.unsqueeze adds an additional dimension to the tensor.\n",
    "        # It indicates the position on where to add the dimension. torch.unsqueeze adds an additional dimension to the tensor.\n",
    "        # So let's say you have a tensor of shape (3), if you add a dimension at the 0 position, it will be of shape (1,3), which means 1 row and 3 columns:\n",
    "        # If you have a 2D tensor of shape (2,2) add add an extra dimension at the 0 position, this will result of the tensor having a shape of (1,2,2), which means one channel, 2 rows and 2 columns. \n",
    "        # If you add at the 1 position, it will be of shape (2,1,2), so it will have 2 channels, 1 row and 2 columns.\n",
    "        # unsqueeze is a method to change the tensor dimensions, such that operations such as tensor multiplication can be possible.\n",
    "        \n",
    "        #create vector of shape seq_len and 1 row  shape (seq_len, 1)\n",
    "        position = torch.arange(0, seq_len, dtype=float).unsqueeze(1) # Creating a tensor of shape (seq_len, 1)\n",
    "        #demominator of the formula is 10000^((2*i)/d_model)\n",
    "        #denominator of the formula is 10000^((2*i)/d_model) here is calculated in log space for numerical stability.\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2).float() * (-math.log(10000.0)/d_model))\n",
    "        #Use sine for even position and cosine for odd position.\n",
    "        #starting from 0 skip every 2 position in the vector to capture even positions.\n",
    "        pe[:,0::2]= torch.sin(position * div_term)\n",
    "        pe[:,1::2]= torch.cos(position * div_term)\n",
    "        \n",
    "        #Add batch dimension to this tensor so that we can apply it to all the batch of sentences.\n",
    "        # Now we just have one tensor/vector of size seq_len * d_model we need to add it to all the batch of sentences.\n",
    "        \n",
    "        #Add new batch dimension to this pe\n",
    "        pe = pe.unsqueeze(0) # This will create a tensor of shape (1, seq_len, d_model)\n",
    "        \n",
    "        #Register this tensor into buffer of this module.\n",
    "        # When we have tensor that we want to keep inside the module, we want to keep the learned parameters saved inside the buffer when we save the file of the module.\n",
    "        #tensor will be saved in the file along with state of he module\n",
    "        self.register_buffer('pe',pe)\n",
    "        \n",
    "    \n",
    "    # Add positional encoding to every word inside the sentence.\n",
    "    def forward(self, x):\n",
    "        #positonal encoding will not be learned by the model.  \n",
    "        # x =x + pe of this particular sentence\n",
    "        #shape (1, seq_len, d_model)\n",
    "        # we do not want to learn positional encoding because they are fixed and cannot be learned by the model. require_grad_(False)\n",
    "        x = x + (self.pe[:,:x.shape[1],:]).require_grad_(False)\n",
    "        #apply dropout to prevent overfitting and return the result.\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer Normalization/add and norm  \n",
    "# If we have a batch of 3 items and each item has features of size 512. \n",
    "# For each item in the batch we calcuate mean and standard deviation of size of vector 512. \n",
    "# Calculate new value for each item by subtracting mean and dividing by standard deviation. \n",
    "# also introduce learnable parameters gamma and beta. Where gamma is multiplied to the new value and beta is added to the new value. \n",
    "# we need model to be able to amplify the value if needed, model will learn to add gamma to the value in such a way that it can amplify the value.\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, epsilon:float=1e-6):\n",
    "        super().__init__()\n",
    "        #epsilon is a small number to avoid division by zero.\n",
    "        self.eps = epsilon\n",
    "        #Using nn.parameter to create learnable parameters.\n",
    "        self.alpha = nn.Parameter(torch.ones(1)) # Multiplied \n",
    "        self.bias = nn.Parameter(torch.zeros(1)) # Added to the new value\n",
    "    \n",
    "    # We need to calculate mean and standard deviation for each item in the batch.\n",
    "    def forward(self, x):\n",
    "        #Usually mean cancels the dimensions that are applied by want to keep it therefore keep_dim =True added.\n",
    "        # Mean of each item column last dimension of the tensor.\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.alpha * (x -mean) / (std + self.eps) + self.bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Feedforward a fully connected layer that the model uses both in encoder and decoder.\n",
    "# This consists of two transformation with ReLU activation function in between. \n",
    "# TWO matrics W1 and W2 are W1 multiplied to X and bias added multiplied by W2 and bias added. \n",
    "# inpiut and output d_model dimension are 512\n",
    "# inner layer has dimensionality of 2048.\n",
    "# FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "# Here max means ReLU activation function on xW1 + b1.\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout:float):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model,d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        # We have input sentence with dimension (Batch, Seq_len, d_model)\n",
    "        #(Batch, Seq_len, d_model) ---> (Batch, Seq_len, d_dff)-->(Batch, Seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-Head Attention takes the input and copy it three times one time as Key, one time as Query and one time as Value.\n",
    "# Input Sequence (seq_len, d_model)\n",
    "# Input Sequence transformed into 3 matrices\n",
    "# H here is number of heads.\n",
    "# Split the martics ALONG THE EMBEDDING DIMENSION NOT ALONG THE SEQ_LEN DIMENSION.\n",
    "# Each HEAD will have FULL SENTENCE BUT DIFFERENT PART OF THE EMBEDDINGS\n",
    "# Apply attention to each head.\n",
    "# Attention(Q,K,V) = softmax(QK^T/sqrt(d_model))V\n",
    "#MultiHead(Q,K,V) = Concat (head_1, head_2,head_3... head_h)W \n",
    "\n",
    "\n",
    "# Q (seq_len, d_model) * Wq (d_model, d_model)  ===> Q'(seq_len, d_model) --Split into H matrices->\n",
    "# K (seq_len, d_model) * Wk (d_model, d_model)  ===> K'(seq_len, d_model) --Split into H matrices->\n",
    "# V (seq_len, d_model) * Wv (d_model, d_model)  ===> V'(seq_len, d_model) --Split into H matrices->\n",
    "# Head = Attention(Q,K,V) = Attention (QWq, KWk, VWv)\n",
    "# H = Concat (head_1, head_2,head_3... head_h)\n",
    "# H * Wo = MH-A\n",
    "#Here Wo is output weight which is h *dv * d_model ===> d_model * d_model\n",
    "#Here dv = d_model/h =d_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    \n",
    "    #h is number of heads\n",
    "    #We need to divide the embedding vectors into h heads\n",
    "    #d_model 512 should be divisible by number of heads.\n",
    "    \n",
    "    def __init__(self, d_model:int, h:int, dropout:float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        assert d_model %h == 0, \"d_model should be divisible by number of heads\"\n",
    "        #d_k is d_model/h\n",
    "        #nn.Linear is a linear layer used in neural networks that applies a linear transformation to input data using weights and biases.\n",
    "        # The nn.Linear module takes two parameters: in_features and out_features, which represent the number of input and output features, \n",
    "        # respectively. When an nn.Linear object is created, it randomly initializes a weight matrix and a bias vector. \n",
    "        # The size of the weight matrix is out_features x in_features, and the size of the bias vector is \n",
    "        # out_features.nn.Linear works by performing a matrix multiplication of the \n",
    "        # input data with the weight matrix and adding the bias term. This operation is applied to each layer in a feed-forward neural network.\n",
    "        # nn.Linear and nn.Conv2d are both fundamental modules in PyTorch used for different purposes. \n",
    "        # While nn.Linear applies a linear transformation to the incoming data, nn.Conv2d applies a 2D convolution over an input signal composed of several input planes.\n",
    "        # nn.Conv2d applies a 2D convolution over an input signal and is primarily used in convolutional layers for tasks like image processing.\n",
    "        self.d_k = d_model // h \n",
    "        self.w_q = nn.Linear(d_model, d_model) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model) # Wv\n",
    "        \n",
    "        # W0 = h * d_k * d_model \n",
    "        self.w_o = nn.Linear(d_model,d_model) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    # Calculate attention call this method without creating instance of this calss\n",
    "    @staticmethod\n",
    "    #Here we get smaller head1, head2, head3...\n",
    "    def attention( query, key, value, mask, dropout:nn.Dropout):\n",
    "        #d_k is the last dimension of query key and value\n",
    "        # attention = Softmax (Q * K^T /sqrt(d_k)) *V\n",
    "        d_k = query.shape[-1]\n",
    "        #transpose last two dimensions transpose(-2,-1)\n",
    "        # last dimension are seq_len by d_k after transpose it becomes d_k by seq_len\n",
    "        # (Batch, h, seq_len, d_k)  ---> (batch, h, seq_len, d_k)\n",
    "        attention_scores = (query @ key.transpose(-2,-1)) /math.sqrt(d_k)\n",
    "        #Before applying Softmax we need to apply mask we need to hide some interactions\n",
    "        #replace values tha we want to mask with very small values\n",
    "        if mask is not None:\n",
    "            # for all attention score where maske == 0 replace with 1e9 very small value -1e9 is like - infinity\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # Input dim (batch, h, seq_len) \n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        \n",
    "        # here second attention_scores is returned for visualization purposes\n",
    "        return (attention_scores @value), attention_scores\n",
    "        \n",
    "        \n",
    "    #if we want some words to not interact with other words we use masking\n",
    "    # The forward() method takes the input data (typically a tensor) and returns the output data as a tensor after passing it through the model's layers.\n",
    "    # The forward pass computes the predicted output tensor, which can be used for making predictions or calculating the loss during training.\n",
    "    # if we do not want some words to interact with other we will set attention to 0.  This is used in decoder to maske words we do not want to decoder to see.\n",
    "    def forward(self,q,k,v, mask):\n",
    "        query = self.w_q(q) # (Batch, seq_len, d_model) * (d_model, d_model) = (Batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (Batch, seq_len, d_model) * (d_model, d_model) = (Batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (Batch, seq_len, d_model) * (d_model, d_model) = (Batch, seq_len, d_model)\n",
    "        \n",
    "        #Divide query key values into h heads \n",
    "        # The \"view\" method functions by altering the tensor's shape while preserving its original dataIn simpler terms, \n",
    "        # it enables developers to adjust the size and structure of the tensor without modifying the information it contains.\n",
    "        # reshaped_x = x.view(2, 4) Here tensor is reshaped to 2 rows and 4 columns. \n",
    "        # Flatten tensor to one dimensional array y.view(-1)\n",
    "        # Adjust batch size: tensor \"images\" with a shape of (32, 3, 64, 64), \n",
    "        # where 32 denotes the batch size, 3 indicates the RGB channels, and 64x64 represents the image size.\n",
    "        \n",
    "        # HWere we kept query.shape[0] and query.shape[1] because we do not want to split the batch size and seq length.\n",
    "        # we want to split by d_model dimension into self.h and self.d_k\n",
    "        # Here self.h * self.d_k = self.d_model\n",
    "        # we need to transpose because We want to have d imension instead of third dimension as second dimension transpose(1,2)\n",
    "        # This will allow each head see seq_length x d_k\n",
    "        # (Batch, Seq_length, d_model) ---> (Batch, Seq_length, h, d_k)  --->(Batch, h, seq_length, d_k)\n",
    "        # We want each head to watch seq_len by d_k\n",
    "        # Each head will see full length of each sentence Seq_length but smaller part of the embedding\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2) # Batch Size x h * Seq_length * d_k\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2) # Batch Size x h * Seq_length * d_k\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2) # Batch Size x h * Seq_length * d_k\n",
    "        \n",
    "        # Calculate the Attention \n",
    "        x, self.attention_scores =  MultiHeadAttentionBlock.attention( query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # We had seq_len in the third dimension, we want seq_length back in the first dimension to combine them.  \n",
    "        #(Batch, h, seq_len, d_k) ---> (Batch, seq_len, h, d_k) --->(Batch, seql_len, d_model)\n",
    "        x = x.transpose(1,2).contigusous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        \n",
    "        #multiply x by Wo output matrix\n",
    "        return self.w_o(x)\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip connections\n",
    "#IT is connection between Add & Norm and the Previous Layer \n",
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self, dropout: float) ->None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNormalization()\n",
    "    \n",
    "    #Here sublayer is the previous layer \n",
    "    def forward(self, x, sublayer):\n",
    "        #Combine x with output of the previous layer or sublayer\n",
    "        #First we apply the normalization then we apply the sublayer\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have Encoder Block which is repeated N times\n",
    "# Ouput of the previous send to next one\n",
    "# Last output send to the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We call it self attention because in the it is applied 3 role query key and value\n",
    "# Encode Block has 2 Add and Norm blocks, one Multi Head Attention and one Feed Forward \n",
    "# Self Attention: \n",
    "#   Self Attention is multi head attention, We call it self attention because in case of Encoder it is applied to the same intput with\n",
    "#  three different roles role of query key and value\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout:float):\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connection = nn.ModuleList([\n",
    "            ResidualConnection(dropout),\n",
    "            ResidualConnection(dropout)\n",
    "        ])\n",
    "    \n",
    "    # src_mask is the mask we want to apply to the input of the encoder\n",
    "    # We want to hide the interaction of padding word with other words\n",
    "    # Self Attention: Sentence that is watching itself. Once sentence interacting with other words in the same sentence.\n",
    "    # Query from decoder watch K and V coming from the encoder\n",
    "    def forward(self, x, src_mask):\n",
    "        # x is sent to the residual_connection skip connection and also to the multi head attention and combine the two.\n",
    "        # call self attention becasue role of q k v is the input itself.\n",
    "        # itis the sentence that is watching iself each word of one sentence is interacting with other word of the same sentence\n",
    "        # In decoder we have cross connection\n",
    "        #Here we are calling forward of the multihead attention block\n",
    "        # Multihead attention combined with x by using residual connection\n",
    "        x = self.residual_connection[0](x, lambda x: self.self_attention_block(x,x,x,src_mask))\n",
    "        x = self.residual_connection[1](x, lambda x: self.feed_forward_block)\n",
    "        return x\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode is made up of upto N encoder objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "        \n",
    "    def forward(self,x, mask):\n",
    "        for layer in self.layers:\n",
    "            #ouput of previous layer become input to the next layer\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \n",
    "    #Self Attention: \n",
    "    # Masked MultHead Attention is the self attention block because same input plays the role of query, key and value.\n",
    "    # Each word in the sentence is mapped to each other word in the same sentence.\n",
    "    # Multi Head Attention: Here query comes from the decoder and key and value coming from the encoder\n",
    "    # This is called Cross attention here we are cross two types of objects.\n",
    "    # Cross Attention: Here query comes from the decoder and key and value coming from the encoder\n",
    "    # Different type of attention blocks added here\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, \n",
    "                 cross_attention_block:MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout:float):\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        # 3 residual connection blocks\n",
    "        self.residual_connection = nn.ModuleList([\n",
    "            ResidualConnection(dropout),\n",
    "            ResidualConnection(dropout),\n",
    "            ResidualConnection(dropout)\n",
    "        ])\n",
    "    \n",
    "    # x is input to the decoder\n",
    "    # and we also need output from the encoder\n",
    "    # here we are dealing with translation so we have source language and target language\n",
    "    #  We have 2 masks one coming from the encoder and one coming from the decoder\n",
    "    # src_mask coming from the encoder source language\n",
    "    # tgt_mask coming from the decoder target language\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        #Calculate self attention first\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x,x,x, src_mask) )\n",
    "        #Here query coming from the decoder and key and value coming from the encoder\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, tgt_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "        \n",
    "    def forward(self,x, encoder_output,src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            #aligned to forward to DecoderBlock\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ouput of mulitihead attention is Linear layer\n",
    "# Output of multihead is seq x d_model \n",
    "# we expect output to be seq x d_model if we do not consider batch dim\n",
    "# We want to map this word back into the vocabulary\n",
    "# We want linear layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear layer convert embedding to position in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    #liner layer projecting from domodel to vocab size\n",
    "    def __init__(self, d_model:int, vocab_size:int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #(Batch, seq_len, d_model) ---> (batch, seq_len, vocab_size)\n",
    "        # apply logsoftmax for numerical stability\n",
    "        return torch.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    #One input embedding for the source language one input embedding for the target language\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed:InputEmbeddings, tgt_embed:InputEmbeddings, src_pos: PositionalEncoding, tgt_pos:PositionalEncoding, projection_layer:ProjectionLayer):\n",
    "        super().__init__()\n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self,x):\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size:int, tgt_vocab_size:int, src_seq_len:int, tgt_seq_len:int, d_model:int=512, N:int=6, h:int=8, dropout:float=.1,d_ff:int=2048):\n",
    "    \n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "    \n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len,dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len,dropout)\n",
    "    \n",
    "    encoder_blocks =[]\n",
    "    \n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block=EncoderBlock(encoder_self_attention_block,feed_forward_block,dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "    \n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "        \n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "    \n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "    \n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "        \n",
    "    return transformer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
